{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b98e58a",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "Hi! this repo is about the udacity navigation project where we will learn an agent to go around and get some bananas. more speicifically the yellow bananas, while the blue one must be avoided!\n",
    "\n",
    "if you want to follow along the first thing you need to do is setup the enviroment by following [this instructions](./Setup.ipynb)\n",
    "\n",
    "![collector](start.gif)\n",
    "\n",
    "I will try to first resolve the enviroment as simple as possible, then introduce some advance techinics to see them in actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d2e17",
   "metadata": {},
   "source": [
    "###### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54be9565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will use  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from unityagents import UnityEnvironment\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"we will use \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254c193",
   "metadata": {},
   "source": [
    "# UnityEnvironment\n",
    "we load the enviroment. the enviroment contains only 1 brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e4d89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.exe\")\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0070c8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space\n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977afcde",
   "metadata": {},
   "source": [
    "# State\n",
    "\n",
    "the state represents 7 rays going out from the agents coded in 5 dimension. the first 4 is what the ray hit (yellow banana, blue banana, wall, nothing) and followed by the distance of the hit object.\n",
    "the last 2 number represent the speed of the agent. \n",
    "\n",
    "![state](img_banana_env_observations.png)\n",
    "\n",
    "---\n",
    "\n",
    "our state is 7 rays * (4 class dimensions, 1 distance dimention) + 2 speed  => 7* 5 + 2 => 37 dimensions state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890386cc",
   "metadata": {},
   "source": [
    "# Actions\n",
    "\n",
    "The avalible actions are 4:\n",
    "\n",
    "- Action 0: Move forward.\n",
    "- Action 1: Move backward.\n",
    "- Action 2: Turn left.\n",
    "- Action 3: Turn right.\n",
    "\n",
    "![action](img_banana_env_actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14927e82",
   "metadata": {},
   "source": [
    "# Vanilla DQN\n",
    "\n",
    "the enviroments looks really simple so let's try with the simpler DQN network I can up with, ie only one hidden layer.\n",
    "the hidden layer shall have a dimension beetween the state and the action dimesion, and since I want the simpler let's say 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76773ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaQNetwork(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, state_size, action_size, units=8):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            units (int): Hidden layer dimension\n",
    "        \"\"\"\n",
    "        super(VanillaQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, units)\n",
    "        self.fc2 = nn.Linear(units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd06988",
   "metadata": {},
   "source": [
    "# Agent\n",
    "if we ingnore all last 6 years of new techinics that's how the agent will look like.\n",
    "the agent will learn from each experience, no buffer, no target, nothing, just simple eval of the tuples: state, action, reward, next state.\n",
    "![dockey](dockey.jpg)\n",
    "will be our dockey agent able to learn walk straight in such condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb34587",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99            # discount factor\n",
    "LR = 5e-4               # learning rate\n",
    "\n",
    "class VanillaAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork = VanillaQNetwork(\n",
    "            state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=LR)\n",
    "\n",
    "    def step(self, state, action, reward, next_state):\n",
    "\n",
    "        states = torch.from_numpy(\n",
    "            np.vstack([state])).float().to(device)\n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([action])).long().to(device)\n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([reward])).float().to(device)\n",
    "        next_states = torch.from_numpy(\n",
    "            np.vstack([next_state])).float().to(device)\n",
    "\n",
    "        self.learn(states, actions, rewards, next_states)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            self.qnetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnetwork(state)\n",
    "            self.qnetwork.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy()).astype(int)\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s') tuples \n",
    "        \"\"\"\n",
    "        # Get max predicted Q values (for next states)\n",
    "        Q_targets_next = self.qnetwork(\n",
    "            next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states\n",
    "        Q_targets = rewards + (GAMMA * Q_targets_next)\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def load(self, file_name):\n",
    "        if file_name is None:\n",
    "            return\n",
    "        self.qnetwork.load_state_dict(torch.load(file_name))\n",
    "\n",
    "    def save(self, file_name):\n",
    "        if file_name is None:\n",
    "            return\n",
    "        torch.save(self.qnetwork.state_dict(), file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926f451",
   "metadata": {},
   "source": [
    "# Training Function\n",
    "next we define our tranning function, I would like to have a tranning function which take a any agent and train it. I also would like to save the result and be able to load it back for some more trainning.\n",
    "As this Enviroment is a continuos task (pick yellow bananas avoid blue bananas) I will completly ingnore the eperiences at the end of the episodes. that would be different if the enviroment was about complete a task.\n",
    "the tranning function need a agent and a function to trasform the env_info to the agent input state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d5950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(agent, state_transform=lambda state: state, file_start=None, file_success=None, file_end=None, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995,):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        state_transform f(env_info): function to transform env_info to agent state input\n",
    "        file_start: file to load the agent\n",
    "        file_end: file to store the agent at the end of training\n",
    "        file_success: file to store the agent when env is resolved\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    agent.load(file_start)             # load agent\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = state_transform(env_info)\n",
    "        score = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state, reward, done = state_transform(\n",
    "                env_info), env_info.rewards[0], env_info.local_done[0]\n",
    "            if not done:\n",
    "                agent.step(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "\n",
    "        message = '\\rEpisode {}\\tAverage Score: {:.2f}\\teps: {:.2f}'.format(\n",
    "            i_episode, np.mean(scores_window), eps)\n",
    "        eps = max(eps_end, eps_decay*eps)  # decrease epsilon\n",
    "        print(message, end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(message)\n",
    "        if np.mean(scores_window) >= 13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(\n",
    "                i_episode-100, np.mean(scores_window)))\n",
    "            agent.save(file_success)\n",
    "            break\n",
    "    agent.save(file_end)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92c7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -0.23\teps: 0.61\n",
      "Episode 200\tAverage Score: 0.99\teps: 0.379\n",
      "Episode 300\tAverage Score: 3.78\teps: 0.22\n",
      "Episode 400\tAverage Score: 6.85\teps: 0.14\n",
      "Episode 500\tAverage Score: 8.27\teps: 0.08\n",
      "Episode 600\tAverage Score: 8.98\teps: 0.05\n",
      "Episode 700\tAverage Score: 10.01\teps: 0.03\n",
      "Episode 800\tAverage Score: 10.61\teps: 0.02\n",
      "Episode 900\tAverage Score: 11.04\teps: 0.01\n",
      "Episode 1000\tAverage Score: 11.45\teps: 0.01\n",
      "Episode 1100\tAverage Score: 11.12\teps: 0.01\n",
      "Episode 1200\tAverage Score: 11.35\teps: 0.01\n",
      "Episode 1300\tAverage Score: 11.01\teps: 0.01\n",
      "Episode 1400\tAverage Score: 11.57\teps: 0.01\n",
      "Episode 1500\tAverage Score: 10.47\teps: 0.01\n",
      "Episode 1600\tAverage Score: 11.98\teps: 0.01\n",
      "Episode 1700\tAverage Score: 11.39\teps: 0.01\n",
      "Episode 1800\tAverage Score: 10.88\teps: 0.01\n",
      "Episode 1900\tAverage Score: 11.50\teps: 0.01\n",
      "Episode 2000\tAverage Score: 11.32\teps: 0.01\n"
     ]
    }
   ],
   "source": [
    "# transform for Vanilla agent\n",
    "def state_transform(env_info):\n",
    "    return env_info.vector_observations[0]\n",
    "# instance Agent\n",
    "agent = VanillaAgent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "# trainning\n",
    "scores = dqn(agent, state_transform=state_transform,\n",
    "             file_start=None, file_end='vanilla_model_2000_episodes.pth', file_success='vanilla_model_success.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d73e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40080f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
